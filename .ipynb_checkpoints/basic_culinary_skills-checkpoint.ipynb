{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\"> Step-1 | Shopping: Know your file formats and data standards</h1></div>\n",
    "\n",
    "Most high-level programming languages have a `native` way for packing and unpacking their own objects:\n",
    "\n",
    "| Language | Format   |\n",
    "| :-------- | :-------  |\n",
    "| MATLAB   | `*.mat`  |\n",
    "| R        | `*.save` |\n",
    "| Python   | `*.pkl`  |\n",
    "| Julia    | `*.jld`  |    \n",
    "\n",
    "\n",
    "\n",
    "<code style=\"background:yellow;color:black\">But when we go to community markets for <code>data shopping</code> to cook a nutritious meal, we usually don't find ingridients available in such formats. </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's say we'd like to bake a `Pythonic pizza üçï`,  and we need tomatoes `üçÖ`   \n",
    "\n",
    "![](assets/tomato.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/gordon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üõí Sunrise Market - Everything you need to throw an audiovisual MRI dinner party\n",
    "\n",
    "![](assets/sunrise_market.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### 1D Data üéº\n",
    "\n",
    "Samples from different MRI sequences (including `NORAH JONES`) and some guitar melodies to compose magnetic melodies in Python. Files are in `*.wav` format, storing audio sampled at `44.1kHz` sampling frequency (or `framerate`) üíø. Respecting Nyquist sampling theorem, this `framerate` allows us to digitize analog sound at the highest frequency audiable to human ear (`20kHz`).    \n",
    "\n",
    "> There are so many Python modules that can load a `*.wav` in a single line, such as `scipy`, `librosa`, `soundfile`, `scikits` etc. \n",
    "\n",
    "For demonstration, we will use Python's standard `wave` library and convert data from `byte` array to `numpy` array. \n",
    "\n",
    "#### 2D Data üåÖ\n",
    "\n",
    "Images acquired using the `NORAH JONES` sequence at 4 TRs. Each image is `100x100`, organized in [Brain Imaging Data Structure (BIDS)](https://bids-specification.readthedocs.io/en/stable/). We will use `pyBIDS` to query and load this data.\n",
    "\n",
    "#### 3D Data üéÜ\n",
    "\n",
    "K-space data acquired using the `NORAH JONES` sequence at 4 TRs. Each file contains `100x100x1x16` data (`16` receive channels), organized in [ISMRM Raw Data - ISMRM-RD](https://github.com/ismrmrd/ismrmrd) format. We will use [`ismrmd-python`](https://github.com/ismrmrd/ismrmrd-python) to load this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\">  Step -2 Mise en place avec <code>NumPy</code></h1></div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\">Load, slice, dice and \"put everything in place\".  </code>\n",
    "\n",
    "![](assets/numpy_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\">  Step -3 Let's welcome our favourite <code>SciPy</code> chefs!</h1></div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\">Python chefs, at your service.</code>\n",
    "\n",
    "![](assets/scipy_chefs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/np_basics.png)\n",
    "\n",
    "### `NumPy` arrays\n",
    "\n",
    "![](assets/np_array.png)\n",
    "\n",
    "Visit the reference [blog post](https://jalammar.github.io/visual-numpy/) for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> If you have `lolviz` and its dependencies intalled properly, `arrayviz` function will show you a visual representation of the array. If not, you will see the `print` output. \n",
    "\n",
    "#### import `numpy` as `np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import NumPy \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A tiny exception block\n",
    "try: \n",
    "    from lolviz import listviz as lolist\n",
    "except:\n",
    "    print('Lolviz is not enabled.')\n",
    "\n",
    "# Pictorial representation of arrays   \n",
    "def arrayviz(inp): \n",
    "    try:\n",
    "        out = lolist(inp)\n",
    "    except:\n",
    "        out = print(inp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Array creation routines\n",
    "`array` `empty` `zeros` `ones` `linspace` `arange` `full` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create an array\n",
    "my_array1 = np.array([11,22,33,44,55,66,77,88])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dtype \n",
    "# ndmin \n",
    "\n",
    "print(\"Number of dimensions: \", my_array.ndim)\n",
    "print(\"Array shape (row,col): \", my_array.shape)\n",
    "print(\"Array size (number of elements): \", my_array.size)\n",
    "print(\"Array data type\", my_array.dtype)\n",
    "print(\"Item size in bytes\", my_array.itemsize)\n",
    "\n",
    "arrayviz(my_array3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`NumPy` provides us with:\n",
    "* A versatile chef's knife to slice our data as we like (slicing)\n",
    "* All the cookware to organize them (data types)\n",
    "* Easy way to access what we need, when we need (indexing)\n",
    "\n",
    "![](assets/np_array2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_array2 = np.arange(11,89,11)\n",
    "\n",
    "print(\n",
    "    \"\\n min: \", my_array2.min(),       # Returns the minimum element itself\n",
    "    \"\\n max: \", my_array2.max(),       # Returns the maximum element itself\n",
    "    \"\\n argmin: \", my_array2.argmin(), # Returns the index of the minimum element in the array\n",
    "    \"\\n argmax: \", my_array2.argmax(), # Returns the index of the maximum element in the array\n",
    "    \"\\n std: \", my_array2.std(),       # Returns the standard deviation of the array \n",
    "    \"\\n sum: \", my_array2.sum(),       # Returns the sum of the elements in the array  \n",
    "    \"\\n prod: \", my_array2.prod(),     # Returns the product of the elements in the array \n",
    "    \"\\n mean: \", my_array2.mean()\n",
    "    \n",
    ")\n",
    "\n",
    "my_array[my_array<50] = 0\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Is `np.max()` any better than `max()` ? \n",
    "\n",
    "To see whether `np.max()` is faster, first we need to create a larger array. Then, we'll use `%timeit` magic to see which one outperforms the other üèé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.rand(10000)\n",
    "\n",
    "print('Numpy aggregation')\n",
    "\n",
    "%timeit -n 500 mx = test.max()\n",
    "\n",
    "print('Python native')\n",
    "\n",
    "%timeit -n 500 mx2 = max(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\"> Ready for our 1D adventure? </h1></div>\n",
    "\n",
    "![](https://www.meme-arsenal.com/memes/74b204e3ff3833101a7efb420816cf36.jpg)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>  1Ô∏è‚É£ Know your data (at least) <strong style=\"color:darkgreen\"><u>at the level you are dealing with it</u></strong> </h3></div>\n",
    "\n",
    "![](assets/wave_np.png)\n",
    "\n",
    "**Please üêª with me here for a while, because we'll be dealing with `*wav` files at a somewhat low-level.** \n",
    "\n",
    "For demonstration, we will write a small function that can read and parse `*.wav` files into a `NumPy` array.\n",
    "\n",
    "Anatomy of a `*.wav` file (boring version üëÄ):\n",
    "\n",
    "![](https://i.stack.imgur.com/GfSNy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<code style=\"background:yellow;color:black\">This step has no coding. Yet, having some insights about your data beforehand gives you a great headstart for the next step! </code>\n",
    "\n",
    "So, [in a manner of speaking](https://www.youtube.com/watch?v=zXhLFb34nz4), semantics **will** do üôÉ If Nouvelle Vague is not your cup of tea, maybe this is:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> <a href=\"https://www.youtube.com/watch?v=AEp08vVYreg\">BOLDplay - Fix it üé∂</a></b> \n",
    "<br>\n",
    "<strong>    \n",
    "<br> When you try your best, but you can't debug\n",
    "<br> When <code>p&lt;0.0000000000000000000000000001</code>, and you are not comfortable with it\n",
    "<br> When you read the code, but it seems OK \n",
    "<br> Stuck in limbo\n",
    "<br>\n",
    "<br> When you smell something fishy about your custom reader \n",
    "<br> When you analyze for days, but it goes to waste\n",
    "<br> What could be worse?    \n",
    "<br>    \n",
    "<br> Data standards will guide you home\n",
    "<br> And validate your data \n",
    "<br> And you won't need to fix it    \n",
    "</strong>        \n",
    "</div>\n",
    "\n",
    "*** \n",
    "\n",
    "üëâ  **`WavMRI/epi.wav` is a stereo (2 channels) recording of the _acoustic noise EPI gradient pulses make_, sampled at `44.1kHz` for `7.42 seconds`. Saved at 16bit depth. This is how I know (Garageband):**\n",
    "\n",
    "![](assets/epi_details.png)\n",
    "\n",
    "$$ nSamples = frameRate \\ (Hz) * duration \\ (s) $$\n",
    "$$ 327547 \\approx 44100 * 7.42 $$\n",
    "\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> Thus, we should expect an array with a length of <code> 655094 = 327547 (samples) X 2 (stereo means 2 channels) </code></code>\n",
    "\n",
    "\n",
    "*** \n",
    "\n",
    "The `number of bytes per sample` is `16(bits)/8 = 2`\n",
    "\n",
    "The `*.wav` file contains these information pieces in the `header`, describing the format of the sound information stored in the `data` chunk: \n",
    "\n",
    "|field name| description | \n",
    "|:---- | :---- |\n",
    "|`channels` | The number of channels in the wave file (mono = 1, stereo = 2) |\n",
    "|`sampwidth`| The number of bytes per sample |\n",
    "|`framerate` | The number of frames per second (a.k.a sampling frequency)|\n",
    "|`nframes` | The number of frames in the data stream (a.k.a number of samples)|\n",
    "|`bytes` | A **string object** containing the bytes of the data stream |\n",
    "\n",
    "$$ dataSize \\ (byte) = nSamples * nChannels * sampWidth $$\n",
    "$$ 327547*2*2 = 1310188 \\ (bytes) \\approx 1.3 \\ (MB) $$\n",
    "\n",
    "\n",
    "![](assets/epi_size.png)\n",
    "\n",
    "$$ TFW \\ what \\ you \\ know \\ about \\ the \\ data \\ makes \\ sense $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> 2Ô∏è‚É£ Mise en place </h3></div>\n",
    "\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> Before coding, we should relate our knowledge about the data with the docs of the module we'll use to read it!</code>\n",
    "<br><br>\n",
    "\n",
    "<details>\n",
    "    <summary> üëâ Click this line to expand the API documentation of <code>wave</code> module. See how objects returned by <code>open</code> can access <code>header</code> and <code>data</code> of a <code>*.wav</code> file! </summary>\n",
    "    <p><img src=\"assets/read_wave_api.png\"></p>\n",
    "</details>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>We did the shopping!</b> \n",
    "<br>    \n",
    "    <br>  <code>*.wav</code>  is still a popular data format among music producers. So we are using fresh ingridients to compose our magnetic melodies, way to go! üë©‚Äçüç≥\n",
    "<br>        \n",
    "</div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> Now it is time to combine our <code>*.wav</code> knowledge with our <code>NumPy</code> skills to create an object that we can <b>hear</b>.</code>\n",
    "\n",
    "![](assets/wavbox.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wave import open as open_wave\n",
    "\n",
    "# Open \"WavMRI/EPI.wav\" file in read mode (open the box)\n",
    "\n",
    "\n",
    "# Use following methods to read metadata (read the invoice)\n",
    "# getnchannels() \n",
    "# getnframes()\n",
    "# getframerate()\n",
    "\n",
    "\n",
    "# Read byte ordered data (take out the content)\n",
    "\n",
    "\n",
    "# Close the file after reading (releases it from memory)\n",
    "\n",
    "\n",
    "# print(\"nChannels: \", nchannels , \"nFrames: \" , nframes, \"framerate: \",framerate, \"stringLength:\", len(z_str))\n",
    "\n",
    "# CREATE NUMPY ARRAY\n",
    "\n",
    "\n",
    "# TAKE MONO CHANNEL (Order is Ch1,Ch2,Ch1,Ch2....)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Time to write some functions\n",
    "\n",
    "***\n",
    "* **`read_wave(filename)`**       Reads \\*.wav file to return: \n",
    "    * `ys`          Audio signal (mono)\n",
    "    * `framerate`   Sampling frequency\n",
    "    * `ts`          Time points\n",
    "***\n",
    "* **`normalize(ys,amp)`**       Normalizes audio signal amplitude to a user defined range (amp): \n",
    "    * `ys`          Normalized audio signal\n",
    "***\n",
    "* **`make_audio(ys,amp)`**      Takes `ys` and `framerate` and returns an eenie meenie media player.\n",
    "***\n",
    "* **`load_audio(filename,rate_factor=1,duration=0,volume=1)`** \n",
    "    * **Inputs**\n",
    "        * filename       Wav file\n",
    "        * rate_factor    Multiplied with sampling freq during `make_audio step`. (>1 higher & faster, vice versa)\n",
    "        * duration       Trims audio [0:duration], yeah just that. \n",
    "        * volume         Normalization range (Use 1 max)\n",
    "    * **Output (dict)**\n",
    "        * `['signal']`   Audio signal \n",
    "        * `['time']`     Time pts\n",
    "        * `['fsamp']`    Sampling frequency\n",
    "        * `['duration']` Shows you the importance of reading docs. Duration of the audio in seconds\n",
    "        * `['play']`     Weeny IPython media player that can play the loaded file\n",
    "***\n",
    "* **`fastforward(ys, factor)`** Speeds up the audio by re-arranging sound samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "def read_wave(filename):\n",
    "    '''\n",
    "    Now we have a tiny function that can\n",
    "    load 16bit wav files into a numpy array.\n",
    "    It also returns its time axis and sampling\n",
    "    frequency.\n",
    "    '''\n",
    "    fp = open_wave(filename, \"r\")\n",
    "    nchannels = fp.getnchannels()\n",
    "    nframes = fp.getnframes()\n",
    "    framerate = fp.getframerate()\n",
    "    z_str = fp.readframes(nframes)\n",
    "    fp.close()\n",
    "    ys = np.frombuffer(z_str, dtype=np.int16)    \n",
    "    if nchannels == 2:\n",
    "        ys = ys[::2]\n",
    "    ts = np.arange(ys.size)/framerate\n",
    "    return ys, framerate,ts\n",
    "\n",
    "def normalize(ys, amp=1.0):\n",
    "    '''\n",
    "    Amplitude range of the *.wav files we load\n",
    "    is random. This function will be useful when\n",
    "    we are combining tracks and would like to hear \n",
    "    some of them less/more in the mix. \n",
    "    '''\n",
    "    # Aggregation (to be)\n",
    "    high, low = abs(max(ys)), abs(min(ys))\n",
    "    # Broadcasting\n",
    "    return amp * ys / max(high, low)\n",
    "\n",
    "def make_audio(ys,frame_rate):\n",
    "    '''\n",
    "    To hear *.wav files we load into np.arrays,\n",
    "    we need to create an IPython Audio object. \n",
    "    '''\n",
    "    audio = Audio(data=ys,rate=frame_rate)\n",
    "    return audio\n",
    "\n",
    "def load_audio(filename,rate_factor=1,duration=0,volume=1):\n",
    "    '''\n",
    "    filename:    *.wav file \n",
    "    rate_factor: Changes sampling frequency at the audio\n",
    "                 generation step. This changes the frequency\n",
    "                 but also impacts the play duration.[0,inf)\n",
    "    duration:    Trims audio at a desired time point (s).                    \n",
    "    volume:      Normalizes signal amplitude [0-1] \n",
    "    '''\n",
    "    ys, fs, ts = read_wave(filename)\n",
    "    if duration is not 0: \n",
    "        # Type casting, broadcasting\n",
    "        cut = np.floor(fs*duration).astype('int')\n",
    "        # Slicing\n",
    "        ys = ys[:cut]\n",
    "        ts = ts[:cut]\n",
    "    ys = normalize(ys,volume)\n",
    "    audio = dict(\n",
    "                signal = ys,\n",
    "                time = ts,\n",
    "                fsamp = fs,\n",
    "                duration = ys.size/fs,\n",
    "                play = make_audio(ys,fs*rate_factor))\n",
    "    return audio\n",
    "\n",
    "def fastforward(ys, factor):\n",
    "    '''\n",
    "    Fast forwards the audio (ys) without messing with the\n",
    "    sampling frequency.\n",
    "    '''\n",
    "    indices = np.round( np.arange(0, ys.size, factor) )\n",
    "    indices = indices[indices < ys.size].astype(int)\n",
    "    return ys[ indices.astype(int) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h3> Do you remember Nokia 3310 music composer? See <a href=\"https://www.youtube.com/watch?v=Iry42Lvcfv4\">this masterpiece.</a></h3> <br> </div>\n",
    "\n",
    "If you don't know what Nokia 3310 is, it means that I'm getting old. Anyway, I just wanted to appreciate the \"sophistication\" in that nostalgic composer. You'll see that adjusting tone, duration & tempo is not that easy. \n",
    "\n",
    "### Jingle gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jingle_gradients(filename, dur):\n",
    "    samp_s = load_audio(filename,rate_factor=1,duration=dur, volume=1)\n",
    "    '''\n",
    "    Change the MRI sample to play Jingle Bells (don't know why)\n",
    "    with a different sequence :)\n",
    "    '''\n",
    "\n",
    "    fake_notes = np.array([1,1,1,1,1,1,1.06**3,\n",
    "                           0.94**4,0.94**2,1,1.06,\n",
    "                           1.06,1.06,1,1,1,0.94**2,\n",
    "                           0.94**2,0.94**2,1,0.94**2,\n",
    "                           1.06**3])\n",
    "    '''\n",
    "    I called them fake because we are changing them by speeding up\n",
    "    the audio signal. This does not easily allow us to set durations \n",
    "    (notes change otherwise).\n",
    "\n",
    "    1.06 is the factor for one halftone up (sharp)\n",
    "    1.06**2 --> 2 halftones up\n",
    "    0.94 is the factor for one halftone down (flat)\n",
    "    0.94**3 --> 3 halftones down etc.\n",
    "    '''\n",
    "    a = fastforward(samp_s['signal'], 1)\n",
    "    for note in fake_notes:\n",
    "        b = fastforward(samp_s['signal'], note)\n",
    "        a = np.hstack((a,b))\n",
    "    return a\n",
    "\n",
    "jingle_ssfp = jingle_gradients('WavMRI/CINE_cartesian_SSFP.wav',0.2)\n",
    "# jingle_epi = jingle_gradients('WavMRI/EPI.wav',0.2)\n",
    "# jingle_beat = jingle_gradients('WavMRI/BEAT.wav',0.2)\n",
    "\n",
    "make_audio(jingle_ssfp,44100)\n",
    "'''\n",
    "If you add up different sequence noise, it won't be harmonic. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h3> üôå Exercise: Amplitude modulation</h3> </div>\n",
    "\n",
    "Generate a pure tone using `np.sin` that has the same duration with a sound sample of your choice, then multiply/add them to see what it gives. You need to unlock the code cell first by achieveing this simple task: \n",
    "\n",
    "```python\n",
    "tduration=\n",
    "'''\n",
    "TASK: Assign tduration with the duration of the signal (in seconds, but be accurate!)\n",
    "'''\n",
    "```\n",
    "<div class=\"alert alert-block alert-warning\"> <h3> Test your musical ear üëÇ</h3><br> Change the <code>sin_freq</code> with small increments, and see after how many Hz you'll distinguish the sine tone from the SPGR noise. 164Hz is the next note (E3). If you can tell them before that, you have an ear for eastern music, if you cannot tell it after 164Hz, you may be tone deaf üëÄ</div>\n",
    "\n",
    "* Set `sin_freq` to 155 (D#3) \n",
    "* Load `3DSPGR_TR12ms.wav`\n",
    "* Use `result = my_sound['signal'] + sin_wave`\n",
    "* Play and see if they are attuned ;)\n",
    "\n",
    "\n",
    " **Hint: You can use this approach to find out the dominant sound frequency of a pulse sequence of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sound = load_audio('WavMRI/3DSPGR_TR12ms.wav') # Select one from WavMRI folder\n",
    "\n",
    "tduration=\n",
    "'''\n",
    "TASK: Assign tduration with the duration of the signal (in seconds, but be accurate!)\n",
    "'''\n",
    "\n",
    "sin_freq = 10\n",
    "'''\n",
    "Choose a lower (e.g. 2) or a higher (e.g. 1000) sin_freq and see how they affect the output.\n",
    "'''\n",
    "\n",
    "sf = my_sound['fsamp'] \n",
    "\n",
    "sin_wave = (np.sin(2*np.pi*np.arange(sf*tduration)*sin_freq/sf))/2\n",
    "'''\n",
    "Playground 1\n",
    "Tip: You can try other trigonometric functions in numpy as well :) \n",
    "'''\n",
    "\n",
    "result = my_sound['signal'] * sin_wave\n",
    "'''\n",
    "Playground 2\n",
    "Tip: Add, subtract & multiply your sound with this sin_wave\n",
    "'''\n",
    "\n",
    "make_audio(result,my_sound['fsamp']) # Hear it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h3> üôå Exercise: Convolution </h3> <br> Ever heard about reverbation? It is a highly popular sound effect for instruments and vocals.</div>\n",
    "\n",
    "> [Reverb](https://blog.landr.com/what-is-reverb/) lets you transport a listener to a concert hall, a cave, a cathedral, or an intimate performance space.\n",
    "\n",
    "Sounds exactly like what we need in 2021. We have some `time domain impulse responses (IR)` that can even take you to outer space (`WavIR/OnAStar.wav`) üåü Wait... What's a time-domain IR? Gunshot, clap, balloon pop... You can imagine how these sounds \"characterize\" their environment. Gunshot in a studio vs gunshot in a large hallway. When you convolve your audio with the former one, you'll get a \"dry\" sound. Convolution with the latter one yields a \"wet\" sound.  \n",
    "\n",
    "#### See the contents of  `WavIR` folder, select one of them, then convolve it with a sound from `WavMRI`, `WavGuitar` or `WavVocal`\n",
    "\n",
    "You will use `np.convolve`: \n",
    "\n",
    "```text\n",
    "Signature: np.convolve(a, v, mode='full')\n",
    "Docstring:\n",
    "Returns the discrete, linear convolution of two one-dimensional sequences.\n",
    "\n",
    "The convolution operator is often seen in signal processing, where it\n",
    "models the effect of a linear time-invariant system on a signal [1]_.  In\n",
    "probability theory, the sum of two independent random variables is\n",
    "distributed according to the convolution of their individual\n",
    "distributions.\n",
    "\n",
    "If `v` is longer than `a`, the arrays are swapped before computation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = load_audio('WavVocal/SUNRISE_BENGU.wav')\n",
    "v = load_audio('WavIr/your_choice_of_IR.wav')\n",
    "\n",
    "'''\n",
    "Perform convolution here, then play it! \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pandas import DataFrame as pd \n",
    "\n",
    "df = pd.from_dict(dict(\n",
    "                    time = epi_time[::50],\n",
    "                    signal = epi[::50]\n",
    "                    ))\n",
    "\n",
    "fig = px.line(df,x='time',y='signal',\n",
    "              template='plotly_dark')\n",
    "fig.update_traces(line=dict(color='limegreen'),opacity=0.7)\n",
    "fig.update_yaxes(range=[-1,1])\n",
    "\n",
    "f2 = go.FigureWidget(fig)\n",
    "\n",
    "@interact(y=(0.0,1.0))\n",
    "def volume_plot(y):\n",
    "    f2.data[0].y = normalize(epi[::50],y)\n",
    "\n",
    "f2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h2> Let's hear NOt RApid Honestly JOyful NEvertheleSs (NORAH JONES) sequence</h2>       \n",
    "</div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\">But first, we will combine the guitar melody (<code>üé∏ WavGuitar/SUNRISE.wav</code>) with vocals üéô artfully recorded by <a href=\"https://soundcloud.com/beng-a\">Bengu Aktas</a> exclusively for this course. Thank you Bengu!   </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal = load_audio('WavVocal/SUNRISE_BENGU.wav')\n",
    "vocal['play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar = load_audio('WavGuitar/SUNRISE_GUITAR.wav')\n",
    "guitar['play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = guitar['signal'].size - vocal['signal'].size\n",
    "vocal_padded = np.pad(vocal['signal'], (0, pad_size), 'constant') # That's why classes are cool \n",
    "make_audio(vocal_padded + guitar['signal'],44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/EOBiyV55MNg',560,315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/dissonance.png)\n",
    "\n",
    "#### There are two problems: \n",
    "\n",
    "1) **Duration** The original duration of the first verse (Sunrise, sunrise, looks like morning in your eyes...) is about 13 seconds (`WavGuitar/SUNRISE.wav`), but NORAH JONES sequence played it for 4 seconds only. Not that slow afterall :)  \n",
    "\n",
    "2) **Frequency** 4 different tones we heard were supposed to match the keynotes of the original chords, but not all of them did. \n",
    "\n",
    "We could have solved the problem 1 with `NumPy` only by splitting the file in 4 parts and time strechting each part. Given that we also need to do some \"tuning\", we'll need `SciPy`.  \n",
    "\n",
    "<code style=\"background:cyan;color:black\"> <b>The following table and color-annotated lyrics of the first verse contain neccesary information to harmoniously remix Sunrise guitar & vocal tracks with the acoustic noise of NORAH JONES sequence made at each TR:</b></code>\n",
    "\n",
    "\n",
    "|Tonic Note  | TR (ms)   |Attuned?|Pitch Shift|Color Code|Wav file|\n",
    "|:-----|:-----------|:--------|:----------------|:----------|:--------|\n",
    "|A#2         | 13.1      |   üò¢   | ‚¨ÜÔ∏è  3 halftones*           | Red      |`3DSPGR_TR13ms.wav`|\n",
    "|D#3         | 12.9      |   üéâ   | -              | Blue     |`3DSPGR_TR12ms.wav`|\n",
    "|C3          | 10.1      |   üéâ   | -              |Purple     |`3DSPGR_TR10ms.wav`|\n",
    "|G#2         | 20        |   üò¢   | ‚¨áÔ∏è  1 halftone  |Green     |`3DSPGR_TR20ms.wav`|\n",
    "\n",
    "\n",
    "> * You fill learn about halftones in the following section.\n",
    "\n",
    "![](assets/sunrise_chords.png)\n",
    "\n",
    "\n",
    "### We'd like to remix something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = load_audio('REMIX/SUNRISE_SPGR_VERSE1.wav')\n",
    "target['play']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> 3Ô∏è‚É£ Bring in the chefs! </h3></div>\n",
    "\n",
    "### Sound stretching using [Phase Vocoder method](http://zulko.github.io/blog/2014/03/29/soundstretching-and-pitch-shifting-in-python/) (Laroche and Dalson 1999)\n",
    "\n",
    "> You first break the sound into overlapping bits, and you rearrange these bits so that they will overlap even more (if you want to shorten the sound) or less (if you want to stretch the sound), like in this figure:\n",
    "\n",
    "![](http://zulko.github.io/images/soundstretching/stretchsound.png)\n",
    "\n",
    "For a more technical reading, see this [article.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1708&rep=rep1&type=pdf)\n",
    "\n",
    "* Divide signal into multiple windows\n",
    "    * We'll take `window_size` as an input, it must be a power of 2 (FFT)\n",
    "* Set an overlapping factor (h)\n",
    "    * Method requires us to create overlapping slices\n",
    "* Select a windowing function \n",
    "    * This [article]([article](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1708&rep=rep1&type=pdf) suggests Hanning, `NumPy` got us covered yet again. \n",
    "* Let user decide stretching factor (f).\n",
    "    * For example, f=0.5 twofold lengthens the signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch(sound_array, f, window_size, h):\n",
    "    \"\"\" Stretches the sound by a factor `f` \"\"\"\n",
    "    phase  = np.zeros(window_size)\n",
    "    hanning_window = np.hanning(window_size)\n",
    "    result = np.zeros( int(sound_array.size//f) + window_size)\n",
    "\n",
    "    for i in np.arange(0, sound_array.size-(window_size+h), int(h*f)): # You already know what np.arange does! \n",
    "        \n",
    "        # Create overlapping arrays \n",
    "        a1 = sound_array[i: i + window_size] # First block \n",
    "        a2 = sound_array[i + h: i + window_size + h] # Shifted block \n",
    "        \n",
    "        # Multiply with Hanning window, then fft \n",
    "        s1 =  np.fft.fft(hanning_window * a1) # First block in freq domain\n",
    "        s2 =  np.fft.fft(hanning_window * a2) # Shifted block in freq domain \n",
    "        '''\n",
    "        ----------------------------------------\n",
    "        This part is really MRI sciency!!!!!  \n",
    "        ---------------------------------------\n",
    "        '''\n",
    "        phase = (phase + np.angle(s2/s1)) % 2*np.pi\n",
    "        '''                                            \n",
    "        (2pi X phase_offset) between overlapping blocks\n",
    "        Note that phase offsets are tracked across windows (SUM), we are in a for loop. This was \n",
    "        supposed to be done more elegantly. Our pitch corrected signal will sound crispy. \n",
    "        ''' \n",
    "        a2_rephased = abs(np.fft.ifft(np.abs(s2)*np.exp(1j*phase)))\n",
    "        '''\n",
    "        If we multiply the shifted frequency block by e^(2pi * i * phase_offset)...\n",
    "        we correct the phase offset, sounds familiar? :))  \n",
    "        '''\n",
    "        # Accumulate/arrange outputs\n",
    "        i2 = int(i/f)\n",
    "        result[i2 : i2 + window_size] += hanning_window*a2_rephased\n",
    "        \n",
    "    result = ((2**(16-4)) * result/result.max()) # normalize (16bit)\n",
    "    return result.astype('int16')\n",
    "\n",
    "def pitchshift(snd_array, n, window_size=2**13, h=2**11):\n",
    "    \"\"\" Changes the pitch of a sound by ``n`` semitones. \"\"\"\n",
    "    factor = 2**(1.0 * n / 12.0)\n",
    "    stretched = stretch(snd_array, 1.0/factor, window_size, h)\n",
    "    return fastforward(stretched[window_size:], factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_me  = load_audio('WavMRI/EPI.wav') \n",
    "pitched = pitchshift(pitch_me['signal'],5)\n",
    "make_audio(pitched,44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A really important Python lesson: If there's something better, just use it :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "vocal  = load_audio('WavVocal/SUNRISE_BENGU.wav')\n",
    "vocal_octave = librosa.effects.pitch_shift(vocal['signal'], vocal['fsamp'], n_steps=12)\n",
    "make_audio(vocal_octave,vocal['fsamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/circle_of_fifths.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_cof = librosa.effects.pitch_shift(vocal['signal'], vocal['fsamp'], n_steps=5)\n",
    "make_audio(vocal['signal'] + vocal_cof,vocal['fsamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [EarSketch](https://earsketch.gatech.edu/landing/#/learn) is a must see!\n",
    "\n",
    "![](assets/ear_sketch.png)\n",
    "\n",
    "#### Create an account, use any wav file you like from this repository and improve your Python skills as you make music with MRI sounds!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "rise": {
   "enable_chalkboard": true,
   "footer": "<h3 style=\"float:right;\">ISMRM</h3>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
