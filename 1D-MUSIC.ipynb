{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\"> Step-1 | Shopping: Know your file formats and data standards</h1></div>\n",
    "\n",
    "Most high-level programming languages have a `native` way for packing and unpacking their own objects:\n",
    "\n",
    "| Language | Format   |\n",
    "| :-------- | :-------  |\n",
    "| MATLAB   | `*.mat`  |\n",
    "| R        | `*.save` |\n",
    "| Python   | `*.pkl`  |\n",
    "| Julia    | `*.jld`  |    \n",
    "\n",
    "\n",
    "\n",
    "<code style=\"background:yellow;color:black\">But when we go to community markets for <code>data shopping</code> to cook a nutritious meal, we usually don't find ingridients available in such formats. </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's say we'd like to bake a `Pythonic pizza üçï`,  and we need tomatoes `üçÖ`   \n",
    "\n",
    "![](assets/tomato.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/gordon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\">  Step -2 Mise en place avec <code>NumPy</code></h1></div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\">Load, slice, dice and \"put everything in place\".  </code>\n",
    "\n",
    "![](assets/numpy_step2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\">  Step -3 Choose your <code>SciPy</code> chefs!</h1></div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\">Python chefs, at your service.</code>\n",
    "\n",
    "![](assets/scipy_chefs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üõí Sunrise Market - Everything you need to throw an audiovisual MRI dinner party\n",
    "\n",
    "![](assets/sunrise_market.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### 1D Data üéº\n",
    "\n",
    "Samples from different MRI sequences (including `NORAH JONES`) and some guitar melodies to compose magnetic melodies in Python. Files are in `*.wav` format, storing audio sampled at `44.1kHz` sampling frequency (or `framerate`) üíø. Respecting Nyquist sampling theorem, this `framerate` allows us to digitize analog sound at the highest frequency audiable to human ear (`20kHz`).    \n",
    "\n",
    "> There are so many Python modules that can load a `*.wav` in a single line, such as `scipy`, `librosa`, `soundfile`, `scikits` etc. \n",
    "\n",
    "For demonstration, we will use Python's standard `wave` library and convert data from `byte` array to `numpy` array. \n",
    "\n",
    "#### 2D Data üåÖ\n",
    "\n",
    "Images acquired using the `NORAH JONES` sequence at 4 TRs. Each image is `100x100`, organized in [Brain Imaging Data Structure (BIDS)](https://bids-specification.readthedocs.io/en/stable/). We will use `pyBIDS` to query and load this data.\n",
    "\n",
    "#### 3D Data üéÜ\n",
    "\n",
    "K-space data acquired using the `NORAH JONES` sequence at 4 TRs. Each file contains `100x100x1x16` data (`16` receive channels), organized in [ISMRM Raw Data - ISMRM-RD](https://github.com/ismrmrd/ismrmrd) format. We will use [`ismrmd-python`](https://github.com/ismrmrd/ismrmrd-python) to load this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/np_basics.png)\n",
    "\n",
    "### `NumPy` arrays\n",
    "\n",
    "![](assets/np_array.png)\n",
    "\n",
    "Visit the reference [blog post](https://jalammar.github.io/visual-numpy/) for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> If you have `lolviz` and its dependencies intalled properly, `arrayviz` function will show you a visual representation of the array. If not, you will see the `print` output. \n",
    "\n",
    "#### import `numpy` as `np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import NumPy \n",
    "import numpy as np   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A tiny exception block\n",
    "try: \n",
    "    from lolviz import listviz as lolist\n",
    "except:\n",
    "    print('Lolviz is not enabled.')\n",
    "\n",
    "# Pictorial representation of arrays   \n",
    "def arrayviz(inp): \n",
    "    try:\n",
    "        out = lolist(inp)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions:  1\n",
      "Array shape (row,col):  (8,)\n",
      "Array size (number of elements):  8\n",
      "Array data type float64\n",
      "Item size in bytes 8\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.47.0 (20210316.0004)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"232pt\" height=\"40pt\"\n",
       " viewBox=\"0.00 0.00 232.00 40.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 36)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-36 228,-36 228,4 -4,4\"/>\n",
       "<!-- node140726691371808 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>node140726691371808</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-width=\"0.5\" points=\"224,-32 0,-32 0,0 224,0 224,-32\"/>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"1,-19 1,-31 29,-31 29,-19 1,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1,-19 29,-19 29,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"12\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"29,-19 29,-31 57,-31 57,-19 29,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"29,-19 57,-19 57,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"40\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">1</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"57,-19 57,-31 85,-31 85,-19 57,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"57,-19 85,-19 85,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">2</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"85,-19 85,-31 113,-31 113,-19 85,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"85,-19 113,-19 113,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">3</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"113,-19 113,-31 141,-31 141,-19 113,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"113,-19 141,-19 141,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"124\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">4</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"141,-19 141,-31 169,-31 169,-19 141,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"141,-19 169,-19 169,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">5</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"169,-19 169,-31 197,-31 197,-19 169,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"169,-19 197,-19 197,-31 \"/>\n",
       "<text text-anchor=\"start\" x=\"180\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">6</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"197,-19 197,-31 223,-31 223,-19 197,-19\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"197,-19 223,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"207\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"9.00\" fill=\"#444443\">7</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"1,-1 1,-19 29,-19 29,-1 1,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"29,-1 29,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"4\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">11.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"29,-1 29,-19 57,-19 57,-1 29,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"57,-1 57,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"32\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">22.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"57,-1 57,-19 85,-19 85,-1 57,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"85,-1 85,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"60\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">33.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"85,-1 85,-19 113,-19 113,-1 85,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"113,-1 113,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"88\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">44.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"113,-1 113,-19 141,-19 141,-1 113,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"141,-1 141,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">55.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"141,-1 141,-19 169,-19 169,-1 141,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"169,-1 169,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"144\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">66.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"169,-1 169,-19 197,-19 197,-1 169,-1\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"197,-1 197,-19 \"/>\n",
       "<text text-anchor=\"start\" x=\"172\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">77.0</text>\n",
       "<polygon fill=\"#fefecd\" stroke=\"transparent\" points=\"197,-1 197,-19 223,-19 223,-1 197,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"199\" y=\"-7.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"11.00\" fill=\"#444443\">88.0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7ffd7c1a6128>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create an array\n",
    "my_array = np.array([11,22,33,44,55,66,77,88])\n",
    "\n",
    "# Other data types\n",
    "# my_array = np.array([11,22,33,44,55,66,77,88],dtype=np.complex64)\n",
    "# my_array = np.array([11,22,33,44,55,66,77,88],dtype=np.float64)\n",
    "\n",
    "# Transpose of a 1D aray has the same shape \n",
    "# my_array = my_array.T\n",
    "\n",
    "print(\"Number of dimensions: \", my_array.ndim)\n",
    "print(\"Array shape (row,col): \", my_array.shape)\n",
    "print(\"Array size (number of elements): \", my_array.size)\n",
    "print(\"Array data type\", my_array.dtype)\n",
    "print(\"Item size in bytes\", my_array.itemsize)\n",
    "\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Array arithmetic operations\n",
    "'''\n",
    "my_array = np.array([11,22,33,44,55,66,77,88])\n",
    "your_array = np.array([1,2,3,4,5,6,7,8]) # Must be the same length with my_array\n",
    "\n",
    "print('Add: ', my_array + your_array,\n",
    "      '\\nSubtract: ', my_array - your_array,\n",
    "      '\\nMultiply: ', my_array * your_array,\n",
    "      '\\nDivide: ', my_array/your_array,\n",
    "      '\\nFloor divide: ', my_array//your_array,\n",
    "      '\\nPower: ', my_array**your_array\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Array creation routines\n",
    "`array` `empty` `zeros` `ones` `linspace` `arange` `full` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_array = np.arange(11,88,11)\n",
    "'''\n",
    "Start from 11 and return values with increments\n",
    "of 11 until 89 is reached. \n",
    "'''\n",
    "print('np.arange: ')\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_array = np.linspace(11,88,5).astype('int16') # Without .astype, it'll return float \n",
    "'''\n",
    "Return 8 evenly spaced numbers over 11-88 interval\n",
    "'''\n",
    "print('np.linspace: ')\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "zrs  = np.zeros(8) # Returns of array of zeros with 8 elements\n",
    "print('np.zeros: ') \n",
    "arrayviz(zrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ones  = np.ones(8).astype('int16') # Returns of array of ones with 8 elements\n",
    "print('np.ones: ')\n",
    "arrayviz(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "full  = np.full(8,10) # Returns of array of desired value with 8 elements\n",
    "print('np.full: ')\n",
    "arrayviz(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "empty  = np.empty(8)\n",
    "'''\n",
    "Returns of array of 8 elements w ithout EXPLICITLY INITIALIZING it with certain values\n",
    "The values we'll see are arbitrary, i.e. as they exist in the memory address allocated for our array.  \n",
    "'''\n",
    "print('np.empty: ')\n",
    "arrayviz(epmty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`NumPy` provides us with:\n",
    "* A versatile chef's knife to slice our data as we like (slicing)\n",
    "* All the cookware to organize them (data types)\n",
    "* Easy way to access what we need, when we need (indexing)\n",
    "\n",
    "![](assets/np_array2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Broadcasting\n",
    "'''\n",
    "my_array = np.arange(11,89,11)\n",
    "\n",
    "my_array2 = my_array # WARNING! This will create a \"shallow\" copy (just another pointer to the same array) \n",
    "\n",
    "my_array2 *= 2 # Equivalent of my_array = my_array * 2\n",
    "arrayviz(my_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "As you can see, changes we made on my_array2 reflected on my_array as well. \n",
    "'''\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To create an \"independent\" clone, we can use deep copy\n",
    "'''\n",
    "import copy\n",
    "my_array = np.arange(11,89,11) # Create a fresh instance  \n",
    "\n",
    "my_array2 = copy.deepcopy(my_array) # Now my_array2 is NOT a pointer to my_array\n",
    "\n",
    "my_array2 *= 2 # // is floor devision operator \n",
    "\n",
    "print('Original: ', my_array, 'Deep-copied, modified: ',my_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Cherrypick an element  \n",
    "'''\n",
    "\n",
    "first = my_array[0]\n",
    "last =  my_array[-1]\n",
    "print('First element: ', first, 'Last element', last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Slice   \n",
    "'''\n",
    "\n",
    "portion_from_start = my_array[:3] # Eqv my_array[0:3]\n",
    "print('First portion: ', portion_from_start)\n",
    "\n",
    "portion_from_end   = my_array[5:] # Eqv my_array[5:-1]\n",
    "print('Last portion: ', portion_from_end)\n",
    "\n",
    "interleave_odd = my_array[::2] # Eqv my_array[0::2]\n",
    "print('Interleaved odd: ', interleave_odd)\n",
    "\n",
    "interleave_even = my_array[1::2]\n",
    "print('Interleaved even: ', interleave_even)\n",
    "\n",
    "reverse = my_array[::-1]\n",
    "print('Reversed: ', reverse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In-place filtering\n",
    "'''\n",
    "my_array = np.arange(11,89,11)\n",
    "\n",
    "my_array[((my_array<50) & (my_array>20))] = 0\n",
    "\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Aggregation functions   \n",
    "'''\n",
    "\n",
    "my_array = np.arange(11,89,11)\n",
    "\n",
    "print(\n",
    "    \"\\n min: \", my_array.min(),       # Returns the minimum element itself\n",
    "    \"\\n max: \", my_array.max(),       # Returns the maximum element itself\n",
    "    \"\\n argmin: \", my_array.argmin(), # Returns the index of the minimum element in the array\n",
    "    \"\\n argmax: \", my_array.argmax(), # Returns the index of the maximum element in the array\n",
    "    \"\\n std: \", my_array.std(),       # Returns the standard deviation of the array \n",
    "    \"\\n sum: \", my_array.sum(),       # Returns the sum of the elements in the array  \n",
    "    \"\\n prod: \", my_array.prod(),     # Returns the product of the elements in the array \n",
    "    \"\\n mean: \", my_array.mean()\n",
    "    \n",
    ")\n",
    "\n",
    "arrayviz(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Is `np.max()` any better than `max()` ? \n",
    "\n",
    "To see whether `np.max()` is faster, first we need to create a larger array. Then, we'll use `%timeit` magic to see which one outperforms the other üèé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test = np.random.rand(10000)\n",
    "'''\n",
    "One important note about random number generation\n",
    "If you'd like that \"randomness\" reproduce, you need to\n",
    "seed numpy random number generator\n",
    "'''\n",
    "np.random.seed(123)\n",
    "\n",
    "print('Numpy aggregation')\n",
    "\n",
    "%timeit -n 500 mx = test.max()\n",
    "\n",
    "print('Python native')\n",
    "\n",
    "%timeit -n 500 mx2 = max(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\"> Get ready for our 1D adventure: Acoustic dissonance </h1></div>\n",
    "\n",
    "<center><img src=\"https://www.meme-arsenal.com/memes/74b204e3ff3833101a7efb420816cf36.jpg\"></center>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>  1Ô∏è‚É£ Know your data (at least) <strong style=\"color:darkgreen\"><u>at the level you are dealing with it</u></strong> </h3></div>\n",
    "\n",
    "<center><img src=\"assets/wave_np.png\"></center>\n",
    "\n",
    "**Please üêª with me here for a while, because we'll be dealing with `*wav` files at a somewhat low-level.** \n",
    "\n",
    "For demonstration, we will write a small function that can read and parse `*.wav` files into a `NumPy` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<code style=\"background:yellow;color:black\">This step has no coding. Yet, having some insights about your data beforehand gives you a great headstart for the next step! </code>\n",
    "\n",
    "So, [in a manner of speaking](https://www.youtube.com/watch?v=zXhLFb34nz4), semantics **will** do üôÉ If Nouvelle Vague is not your cup of tea, maybe this is:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b> <a href=\"https://www.youtube.com/watch?v=AEp08vVYreg\">BOLDplay - Fix it üé∂</a></b> \n",
    "<br>\n",
    "<strong>    \n",
    "<br> When you try your best, but you can't debug\n",
    "<br> When <code>p&lt;0.0000000000000000000000000001</code>, and you are not comfortable with it\n",
    "<br> When you read the code, but it seems OK \n",
    "<br> Stuck in limbo\n",
    "<br>\n",
    "<br> When you smell something fishy about your custom reader \n",
    "<br> When you analyze for days, but it goes to waste\n",
    "<br> What could be worse?    \n",
    "<br>    \n",
    "<br> Data standards will guide you home\n",
    "<br> And validate your data \n",
    "<br> And you won't need to fix it    \n",
    "</strong>        \n",
    "</div>\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "üëâ  **`WavMRI/EPI.wav` is a stereo (2 channels) recording of the _acoustic noise EPI gradient pulses make_, sampled at `44.1kHz` for `7.42 seconds`. Saved at 16bit depth. This is how I know (Garageband):**\n",
    "\n",
    "<center><img src=\"assets/epi_details.png\"></center>\n",
    "\n",
    "$$ nSamples = frameRate \\ (Hz) * duration \\ (s) $$\n",
    "$$ 327547 \\approx 44100 * 7.42 $$\n",
    "\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> Thus, we should expect an array with a length of <code> 655094 = 327547 (samples) X 2 (stereo means 2 channels) </code></code>\n",
    "\n",
    "\n",
    "*** \n",
    "\n",
    "The `number of bytes per sample` is `16(bits)/8 = 2`\n",
    "\n",
    "The `*.wav` file contains these information pieces in the `header`, describing the format of the sound information stored in the `data` chunk: \n",
    "\n",
    "|field name| description | \n",
    "|:---- | :---- |\n",
    "|`channels` | The number of channels in the wave file (mono = 1, stereo = 2) |\n",
    "|`sampwidth`| The number of bytes per sample |\n",
    "|`framerate` | The number of frames per second (a.k.a sampling frequency)|\n",
    "|`nframes` | The number of frames in the data stream (a.k.a number of samples)|\n",
    "|`bytes` | A **string object** containing the bytes of the data stream |\n",
    "\n",
    "$$ dataSize \\ (byte) = nSamples * nChannels * sampWidth $$\n",
    "$$ 327547*2*2 = 1310188 \\ (bytes) \\approx 1.3 \\ (MB) $$\n",
    "\n",
    "\n",
    "<center><img src=\"assets/epi_size.png\"></center>\n",
    "\n",
    "$$ TFW \\ what \\ you \\ know \\ about \\ the \\ data \\ makes \\ sense $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> 2Ô∏è‚É£ Mise en place </h3>\n",
    "<br>    \n",
    "<b>We did the shopping!</b> \n",
    "<br>    \n",
    "    <br>  <code>*.wav</code>  is still a popular data format among music producers. So we are using fresh ingridients to compose our magnetic melodies, way to go! üë©‚Äçüç≥\n",
    "<br>        \n",
    "</div>\n",
    "\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> Now it is time to combine our <code>*.wav</code> knowledge with our <code>NumPy</code> skills to create an object that we can <b>hear</b>.</code>\n",
    "\n",
    "<center><img src=\"assets/wavbox.png\"></center>\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> Before coding, we should relate our knowledge about the data with the docs of the module we'll use to read it!</code>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<details>\n",
    "    <summary> üëâ Click this line to expand the API documentation of <code>wave</code> module. See how objects returned by <code>open</code> can access <code>header</code> and <code>data</code> of a <code>*.wav</code> file! </summary>\n",
    "    <p><img src=\"assets/read_wave_api.png\"></p>\n",
    "</details>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from wave import open as open_wave\n",
    "\n",
    "'''\n",
    "OPEN THE BOX \n",
    "Open \"WavMRI/EPI.wav\" file in read mode\n",
    "'''\n",
    "\n",
    "fp = open_wave(\"WavMRI/EPI.wav\", \"r\")\n",
    "\n",
    "'''\n",
    "READ THE INVOICE\n",
    "Use wave module's methods to read metadata \n",
    "'''\n",
    "nchannels = fp.getnchannels()\n",
    "nframes = fp.getnframes()\n",
    "framerate = fp.getframerate()\n",
    "sampwidthbyte = fp.getsampwidth()\n",
    "\n",
    "print(\"nChannels: \", nchannels , \"\\nnFrames: \" , nframes, \"\\nframerate: \",framerate, \"\\nsampwidth in bytes: \",sampwidthbyte)\n",
    "\n",
    "'''\n",
    "TAKE OUT THE CONTENTS\n",
    "\n",
    "We read machine-byte ordered data into\n",
    "AN ARRAY OF BYTES (STRING ENCODED)\n",
    "'''\n",
    "z_str = fp.readframes(nframes)\n",
    "\n",
    "'''\n",
    "CLOSE THE BOX \n",
    "''' \n",
    "fp.close()\n",
    "\n",
    "'''\n",
    "SEE WHAT IT LOOKS LIKE \n",
    "\n",
    "Expected number of frames per channel was 327547 (fSamp * duration)\n",
    "Expected content size was about 1.3MB \n",
    "'''\n",
    "print( \"BOX CONTENT SIZE\", len(z_str), '\\nBOX CONTENT: ', z_str[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "MAKE THE CONTENT USEFUL BASED ON WHAT WE LEARNT FROM THE INVOICE!\n",
    "We already know that the data is recorded in 16bit. But we can also \n",
    "infer it from the metadata \n",
    "'''\n",
    "\n",
    "print('Each entry is ', sampwidthbyte * 8, ' bits. (1 byte = 8 bits)')\n",
    "\n",
    "'''\n",
    "CONVERT DATA INTO USEFUL INFORMATION\n",
    "\n",
    "We can't use np.array, we need to use \n",
    "frombuffer as we are reading machine-byte \n",
    "ordered data. \n",
    "'''\n",
    "sgnl = np.frombuffer(z_str, dtype=np.int16)    \n",
    "\n",
    "'''\n",
    "PLOTLY EXPRESS \n",
    "One line of code for interactive plots!  \n",
    "NOTE that I plot 1 in every 50 samples. This is just to make things faster. \n",
    "'''\n",
    "import plotly.express as px\n",
    "px.line(sgnl[::50],template='plotly_dark',title='STEREO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TAKE OUT ONLY ONE CHANNEL\n",
    "Order: Ch1, Ch2, Ch1, Ch2....\n",
    "\n",
    "SLICE \n",
    "'''\n",
    "sgnl = sgnl[::2]\n",
    "\n",
    "# Again, plotting 1/50 samples\n",
    "px.line(sgnl[::50],template='plotly_dark',title='MONO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "MAKE AUDIO\n",
    "Use IPython's Audio module\n",
    "'''\n",
    "from IPython.display import Audio\n",
    "Audio(data=sgnl,rate=framerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Time to write some functions\n",
    "\n",
    "***\n",
    "* **`read_wave(filename)`**       Reads \\*.wav file to return: \n",
    "    * `ys`          Audio signal (mono)\n",
    "    * `framerate`   Sampling frequency\n",
    "    * `ts`          Time points\n",
    "***\n",
    "* **`normalize(ys,amp)`**       Normalizes audio signal amplitude to a user defined range (amp): \n",
    "    * `ys`          Normalized audio signal\n",
    "***\n",
    "* **`make_audio(ys,amp)`**      Takes `ys` and `framerate` and returns an eenie meenie media player.\n",
    "***\n",
    "* **`load_audio(filename,rate_factor=1,duration=0,volume=1)`** \n",
    "    * **Inputs**\n",
    "        * filename       Wav file\n",
    "        * rate_factor    Multiplied with sampling freq during `make_audio step`. (>1 higher & faster, vice versa)\n",
    "        * duration       Trims audio [0:duration], yeah just that. \n",
    "        * volume         Normalization range (Use 1 max)\n",
    "    * **Output (dict)**\n",
    "        * `['signal']`   Audio signal \n",
    "        * `['time']`     Time pts\n",
    "        * `['fsamp']`    Sampling frequency\n",
    "        * `['duration']` Shows you the importance of reading docs. Duration of the audio in seconds\n",
    "        * `['play']`     Weeny IPython media player that can play the loaded file\n",
    "***\n",
    "* **`fastforward(ys, factor)`** Speeds up the audio by re-arranging sound samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def read_wave(filename):\n",
    "    '''\n",
    "    Now we have a tiny function that can\n",
    "    load 16bit wav files into a numpy array.\n",
    "    It also returns its time axis and sampling\n",
    "    frequency.\n",
    "    '''\n",
    "    fp = open_wave(filename, \"r\")\n",
    "    nchannels = fp.getnchannels()\n",
    "    nframes = fp.getnframes()\n",
    "    framerate = fp.getframerate()\n",
    "    z_str = fp.readframes(nframes)\n",
    "    fp.close()\n",
    "    # We assume that data is always 16bits. Change this to\n",
    "    # int64 and see what happens :) \n",
    "    ys = np.frombuffer(z_str, dtype=np.int16)    \n",
    "    if nchannels == 2:\n",
    "        ys = ys[::2]\n",
    "    ts = np.arange(ys.size)/framerate\n",
    "    return ys, framerate,ts\n",
    "\n",
    "def normalize(ys, amp=1.0):\n",
    "    '''\n",
    "    Amplitude range of the *.wav files we load\n",
    "    is random. This function will be useful when\n",
    "    we are combining tracks and would like to hear \n",
    "    some of them less/more in the mix. \n",
    "    '''\n",
    "    # TASK: Use NumPy aggregation functions :) \n",
    "    high, low = abs(max(ys)), abs(min(ys))\n",
    "    # Broadcasting\n",
    "    return amp * ys / max(high, low)\n",
    "\n",
    "def make_audio(ys,frame_rate):\n",
    "    '''\n",
    "    To hear *.wav files we load into np.arrays,\n",
    "    we need to create an IPython Audio object. \n",
    "    '''\n",
    "    audio = Audio(data=ys,rate=frame_rate)\n",
    "    return audio\n",
    "\n",
    "def load_audio(filename,rate_factor=1,duration=0,volume=1):\n",
    "    '''\n",
    "    filename:    *.wav file \n",
    "    rate_factor: Changes sampling frequency at the audio\n",
    "                 generation step. This changes the frequency\n",
    "                 but also impacts the play duration.[0,inf)\n",
    "    duration:    Trims audio at a desired time point (s).                    \n",
    "    volume:      Normalizes signal amplitude [0-1] \n",
    "    '''\n",
    "    ys, fs, ts = read_wave(filename)\n",
    "    if duration is not 0: \n",
    "        # Type casting, broadcasting\n",
    "        cut = np.floor(fs*duration).astype('int')\n",
    "        # Slicing\n",
    "        ys = ys[:cut]\n",
    "        ts = ts[:cut]\n",
    "    ys = normalize(ys,volume)\n",
    "    audio = dict(\n",
    "                signal = ys,\n",
    "                time = ts,\n",
    "                fsamp = fs,\n",
    "                duration = ys.size/fs,\n",
    "                play = make_audio(ys,fs*rate_factor))\n",
    "    return audio\n",
    "\n",
    "def fastforward(ys, factor):\n",
    "    '''\n",
    "    Fast forwards the audio (ys) without messing with the\n",
    "    sampling frequency.\n",
    "    '''\n",
    "    idx = np.round( np.arange(0, ys.size, factor) )\n",
    "    idx = idx[idx < ys.size].astype(int)\n",
    "    return ys[ idx.astype(int) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A really simple interactive visualization\n",
    "'''\n",
    "    \n",
    "import plotly.graph_objects as go \n",
    "from pandas import DataFrame as pd \n",
    "from ipywidgets import interact \n",
    "\n",
    "'''\n",
    "USE load_audio\n",
    "'''\n",
    "temp = load_audio('WavMRI/EPI.wav')\n",
    "\n",
    "df = pd.from_dict(dict(time = temp['time'][::50], signal = temp['signal'][::50]))\n",
    "\n",
    "fig = px.line(df,x='time',y='signal',\n",
    "              template='plotly_dark', title=\"EPI acoustic noise, Normalized\")\n",
    "\n",
    "fig.update_traces(line=dict(color='limegreen'),opacity=0.7)\n",
    "fig.update_yaxes(range=[-1,1])\n",
    "\n",
    "f2 = go.FigureWidget(fig)\n",
    "\n",
    "@interact(y=(0.0,1.0))\n",
    "def volume_plot(y):\n",
    "    '''\n",
    "    CALL normalize each time the slider is moved\n",
    "    '''\n",
    "    f2.data[0].y = normalize(temp['signal'][::50],y)\n",
    "\n",
    "f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h3> Do you remember Nokia 3310 music composer? See <a href=\"https://www.youtube.com/watch?v=Iry42Lvcfv4\">this masterpiece.</a></h3> <br> </div>\n",
    "\n",
    "If you don't know what Nokia 3310 is, it means that I'm getting old. Anyway, I just wanted to appreciate the \"sophistication\" in that nostalgic composer. You'll see that adjusting tone, duration & tempo is not that easy. \n",
    "\n",
    "### Jingle gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def jingle_gradients(filename, dur):\n",
    "    samp_s = load_audio(filename,rate_factor=1,duration=dur, volume=1)\n",
    "    \n",
    "    # Be creative! \n",
    "    fake_notes = np.array([1,1,1,1,1,1,1.06**3,\n",
    "                           0.94**4,0.94**2,1,1.06,\n",
    "                           1.06,1.06,1,1,1,0.94**2,\n",
    "                           0.94**2,0.94**2,1,0.94**2,\n",
    "                           1.06**3])\n",
    "    '''\n",
    "    I called them fake because we are changing them by speeding up\n",
    "    the audio signal. This does not easily allow us to set durations \n",
    "    (notes change otherwise).\n",
    "\n",
    "    1.06 is the factor for one halftone up (sharp)\n",
    "    1.06**2 --> 2 halftones up\n",
    "    0.94 is the factor for one halftone down (flat)\n",
    "    0.94**3 --> 3 halftones down etc.\n",
    "    '''\n",
    "    a = fastforward(samp_s['signal'], 1)\n",
    "    for note in fake_notes:\n",
    "        b = fastforward(samp_s['signal'], note)\n",
    "        a = np.hstack((a,b))\n",
    "    return a\n",
    "\n",
    "'''\n",
    "Change the MRI sample to play Jingle Bells \n",
    "with a different sequence :)\n",
    "'''\n",
    "    \n",
    "#jingle = jingle_gradients('WavMRI/CINE_cartesian_SSFP.wav',0.2)\n",
    "jingle = jingle_gradients('WavMRI/EPI.wav',0.2)\n",
    "#jingle = jingle_gradients('WavMRI/BEAT.wav',0.2)\n",
    "\n",
    "'''\n",
    "If you add up different sequence noise, it won't be harmonic. \n",
    "'''\n",
    "make_audio(jingle,44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Amplitude modulation\n",
    "'''\n",
    "\n",
    "def f(t, f_c, f_m, beta):\n",
    "    '''\n",
    "    t    = time\n",
    "    f_c  = carrier frequency\n",
    "    f_m  = modulation frequency\n",
    "    beta = modulation index\n",
    "    '''\n",
    "    return np.sin(2*np.pi*f_c*t - beta*np.cos(2*f_m*np.pi*t))\n",
    "\n",
    "def to_integer(signal):\n",
    "    '''\n",
    "    Take samples in [-1, 1] and scale to 16-bit integers,\n",
    "    values between -2^15 and 2^15 - 1.\n",
    "    '''\n",
    "    return np.int16(signal*(2**15 - 1))\n",
    "\n",
    "N = 44100\n",
    "x = np.arange(3*N) # three seconds of audio\n",
    "\n",
    "siren = f(x/N, 1500, 3, 100)\n",
    "make_audio(siren,44100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h3> üôå Exercise: Wav signal & pure tones </h3> </div>\n",
    "\n",
    "Generate a pure tone using `np.sin` that has the same duration with a sound sample of your choice, then multiply/add them to see what it gives. You need to unlock the code cell first by achieveing this simple task: \n",
    "\n",
    "```python\n",
    "tduration=\n",
    "'''\n",
    "TASK: Assign tduration with the duration of the signal (in seconds, but be accurate!)\n",
    "'''\n",
    "```\n",
    "<div class=\"alert alert-block alert-warning\"> <h3> Test your musical ear üëÇ</h3><br> Change the <code>sin_freq</code> with small increments, and see after how many Hz you'll distinguish the sine tone from the SPGR noise. 164Hz is the next note (E3). If you can tell them before that, you have an ear for eastern music, if you cannot tell it after 164Hz, you may be tone deaf üëÄ</div>\n",
    "\n",
    "* Set `sin_freq` to 155 (D#3) \n",
    "* Load `3DSPGR_TR12ms.wav`\n",
    "* Use `result = my_sound['signal'] + sin_wave`\n",
    "* Play and see if they are attuned ;)\n",
    "\n",
    "\n",
    " **Hint: You can use this approach to find out the dominant sound frequency of a pulse sequence of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "my_sound = load_audio('WavMRI/3DSPGR_TR12ms.wav') # Select one from WavMRI folder\n",
    "\n",
    "tduration=\n",
    "'''\n",
    "TASK: Assign tduration with the duration of the signal (in seconds, but be accurate!)\n",
    "'''\n",
    "\n",
    "sin_freq = 10\n",
    "'''\n",
    "Choose a lower (e.g. 2) or a higher (e.g. 1000) sin_freq and see how they affect the output.\n",
    "'''\n",
    "\n",
    "sf = my_sound['fsamp'] \n",
    "\n",
    "sin_wave = (np.sin(2*np.pi*np.arange(sf*tduration)*sin_freq/sf))/2\n",
    "'''\n",
    "Playground 1\n",
    "Tip: You can try other trigonometric functions in numpy as well :) \n",
    "'''\n",
    "\n",
    "result = my_sound['signal'] * sin_wave\n",
    "'''\n",
    "Playground 2\n",
    "Tip: Add, subtract & multiply your sound with this sin_wave\n",
    "'''\n",
    "\n",
    "make_audio(result,my_sound['fsamp']) # Hear it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h3> üôå Exercise: Convolution </h3> <br> Ever heard about reverbation? It is a highly popular sound effect for instruments and vocals.</div>\n",
    "\n",
    "> [Reverb](https://blog.landr.com/what-is-reverb/) lets you transport a listener to a concert hall, a cave, a cathedral, or an intimate performance space.\n",
    "\n",
    "Sounds exactly like what we need in 2021. We have some `time domain impulse responses (IR)` that can even take you to outer space (`WavIR/OnAStar.wav`) üåü Wait... What's a time-domain IR? Gunshot, clap, balloon pop... You can imagine how these sounds \"characterize\" their environment. Gunshot in a studio vs gunshot in a large hallway. When you convolve your audio with the former one, you'll get a \"dry\" sound. Convolution with the latter one yields a \"wet\" sound.  \n",
    "\n",
    "#### See the contents of  `WavIR` folder, select one of them, then convolve it with a sound from `WavMRI`, `WavGuitar` or `WavVocal`\n",
    "\n",
    "You will use `np.convolve`: \n",
    "\n",
    "```text\n",
    "Signature: np.convolve(a, v, mode='full')\n",
    "Docstring:\n",
    "Returns the discrete, linear convolution of two one-dimensional sequences.\n",
    "\n",
    "The convolution operator is often seen in signal processing, where it\n",
    "models the effect of a linear time-invariant system on a signal [1]_.  In\n",
    "probability theory, the sum of two independent random variables is\n",
    "distributed according to the convolution of their individual\n",
    "distributions.\n",
    "\n",
    "If `v` is longer than `a`, the arrays are swapped before computation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "a = load_audio('WavVocal/SUNRISE_BENGU.wav')\n",
    "# v = load_audio('WavIR/your_choice_of_IR.wav')\n",
    "\n",
    "'''\n",
    "Perform convolution here, then play it! \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2> Let's hear NOt RApid Honestly JOyful NEvertheleSs (NORAH JONES) sequence</h2>       \n",
    "</div>\n",
    "\n",
    "<code style=\"background:yellow;color:black\">But first, we will combine the guitar melody (<code>üé∏ WavGuitar/SUNRISE.wav</code>) with vocals üéô artfully recorded by <a href=\"https://soundcloud.com/beng-a\">Bengu Aktas</a> exclusively for this course. Thank you Bengu!   </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocal = load_audio('WavVocal/SUNRISE_BENGU.wav')\n",
    "vocal['play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "guitar = load_audio('WavGuitar/SUNRISE_GUITAR.wav')\n",
    "guitar['play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Remember array arithmetics, arrays should be of equal size\n",
    "We can use np.padding on vocal \n",
    "'''\n",
    "pad_size = guitar['signal'].size - vocal['signal'].size # Find out how much longer is the guitar track \n",
    "vocal_padded = np.pad(vocal['signal'], (0, pad_size), 'constant') # Add that many zeros at the end of the vocal track\n",
    "\n",
    "make_audio(vocal_padded + guitar['signal'],44100) # Make a new audio by adding them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Time to hear the acoustic noise NORAH JONES sequence makes and hear (imaginary) feedback from Norah Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/EOBiyV55MNg',560,315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](assets/dissonance.png)\n",
    "\n",
    "#### There are two problems: \n",
    "\n",
    "1) **Duration** The original duration of the first verse (Sunrise, sunrise, looks like morning in your eyes...) is about 13 seconds (`WavGuitar/SUNRISE.wav`), but NORAH JONES sequence played it for 4 seconds only. Not that slow afterall :)  \n",
    "\n",
    "2) **Frequency** 4 different tones we heard were supposed to match the keynotes of the original chords, but not all of them did. \n",
    "\n",
    "\n",
    "## The beauty of music is in the ear of the beholder. \n",
    "\n",
    "### Python knowledge + MRI noise + Vocal + Guitar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "target = load_audio('REMIX/SUNRISE_SPGR_VERSE1.wav')\n",
    "target['play']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> 3Ô∏è‚É£ From the time to the frequency domain: Bring in the <code>SciPy</code> chefs, we need them now! </h3></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.fft import fftshift, fft, ifftshift, ifft\n",
    "import scipy.fftpack as fftpack\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FREQUENCY SPECTRUM\n",
    "fftshift(fft(mri_orig['signal'])) --> 1D FFT \n",
    "fftshift(fftpack.fftfreq(mri_orig['signal'].size)) --> Sample frequencies\n",
    "'''\n",
    "\n",
    "mri_orig  = load_audio('WavMRI/EPI.wav') \n",
    "'''\n",
    "Try other sequences!\n",
    "BEAT.wav\n",
    "AFI.wav \n",
    "LOC_siemens.wav\n",
    "LOC_SSFP.wav\n",
    "RT_Shim.wav \n",
    "...\n",
    "'''\n",
    "\n",
    "from pandas import DataFrame as pd\n",
    "\n",
    "# 1D FFT\n",
    "X = fftshift(fft(mri_orig['signal']))\n",
    "\n",
    "# Sample frequencies \n",
    "freqs = fftshift(fftpack.fftfreq(mri_orig['signal'].size)) * mri_orig['fsamp']\n",
    "\n",
    "df = pd.from_dict(dict(\n",
    "                    Frequency = freqs,\n",
    "                    Magnitude = X.real,\n",
    "                    Phase = np.angle(X)/np.pi\n",
    "                    ))\n",
    "\n",
    "# Real component (yellow)\n",
    "# You can change y='Phase to show phase instead'\n",
    "fig = px.line(df,x='Frequency',y='Magnitude',\n",
    "              template='plotly_dark', title=\"Frequency Spectrum (0-20kHz as fs is 44.1kHz), please zoom in.\")\n",
    "\n",
    "fig.update_traces(line=dict(color='yellow'),opacity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "POWER SPECTRAL DENSITY \n",
    "\n",
    "Welch‚Äôs method computes an estimate of the power spectral density by dividing \n",
    "the data into overlapping segments, computing a modified periodogram \n",
    "for each segment and averaging the periodograms.\n",
    "\n",
    "'''\n",
    "\n",
    "f, psd = signal.welch(mri_orig['signal'], mri_orig['fsamp'], 'flattop', 1024, scaling='spectrum',return_onesided=False)\n",
    "\n",
    "df = pd.from_dict(dict(\n",
    "                    Frequency = f,\n",
    "                    PSD = psd\n",
    "                    ))\n",
    "\n",
    "fig = px.line(df,x='Frequency',y='PSD',\n",
    "              template='plotly_dark', title=\"Power Spectral Density (0-20kHz as fs is 44.1kHz), please zoom in.\")\n",
    "fig.update_traces(line=dict(color='pink'),opacity=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SPECTOGRAM\n",
    "Spectrograms can be used as a way of visualizing the change of \n",
    "a nonstationary signal‚Äôs frequency content over time.\n",
    "'''\n",
    "\n",
    "f, t, Sxx = signal.spectrogram(mri_orig['signal'], mri_orig['fsamp'],scaling='spectrum')\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z=Sxx,\n",
    "        x=t,\n",
    "        y=f,\n",
    "        colorscale='Viridis'))\n",
    "fig.update_xaxes(title = 'Time (s)')\n",
    "fig.update_yaxes(title = 'Frequency (Hz)')\n",
    "fig.update_layout(title = 'Spectogram')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2> Go-to effect of all musicians: Reverb </h2>       \n",
    "</div>\n",
    "\n",
    "### The exercise above asked you to perform convolution in time domain to achieve this effect. Here' well do the same thing by multiplication in the frequency domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar  = load_audio('WavGuitar/SUNRISE_GUITAR.wav')\n",
    "\n",
    "star  = load_audio('WavIR/OnAStar.wav')\n",
    "'''\n",
    "Try other reverb IRs: \n",
    "LargeLongEchoHall.wav\n",
    "Rays.wav\n",
    "VocalDuo.wav\n",
    "'''\n",
    "\n",
    "def zero_pad(x, k):\n",
    "    '''\n",
    "    Inputs must be of the same size.  \n",
    "    Exercise: You can also use np.pad :) \n",
    "    '''\n",
    "    return np.append(x, np.zeros(k))\n",
    "\n",
    "# Zero padding\n",
    "G, S = guitar['signal'].size, star['signal'].size\n",
    "g_zp = zero_pad(guitar['signal'], S-1)\n",
    "s_zp = zero_pad(star['signal'], G-1)\n",
    "\n",
    "# FFT for guitar \n",
    "X = fft(g_zp)\n",
    "# Inverse (real) FFT guitar after multiplying it with the FFT of the reverb IR \n",
    "output = ifft(X * fft(s_zp)).real\n",
    "# Depending on the IR, output can be longer, it'll become clearer when you hear it\n",
    "output = output + g_zp\n",
    "\n",
    "# Let's hear! \n",
    "make_audio(output,guitar['fsamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge: Maintain the tempo, modulate the pitch\n",
    "\n",
    "### Sound stretching using [Phase Vocoder method](http://zulko.github.io/blog/2014/03/29/soundstretching-and-pitch-shifting-in-python/) (Laroche and Dalson 1999)\n",
    "\n",
    "> You first break the sound into overlapping bits, and you rearrange these bits so that they will overlap even more (if you want to shorten the sound) or less (if you want to stretch the sound), like in this figure:\n",
    "\n",
    "<center><img src=\"http://zulko.github.io/images/soundstretching/stretchsound.png\"></center>\n",
    "\n",
    "For a more technical reading, see this [article.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1708&rep=rep1&type=pdf)\n",
    "\n",
    "* Divide signal into multiple windows\n",
    "    * We'll take `window_size` as an input, it must be a power of 2 (FFT)\n",
    "* Set an overlapping factor (h) a.k.a. \"hop size\"\n",
    "    * Method requires us to create overlapping slices\n",
    "* Select a windowing function \n",
    "    * This [article]([article](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1708&rep=rep1&type=pdf) suggests Hanning, `NumPy` got us covered yet again. \n",
    "* Let user decide stretching factor (f).\n",
    "    * For example, f=0.5 twofold lengthens the signal\n",
    "\n",
    "## Daft Punk is known for their famous Vocoder effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/x84m3YyO2oU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb35e2097f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/x84m3YyO2oU',560,315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def stretch(sound_array, f, window_size, h):\n",
    "    \"\"\" Stretches the sound by a factor `f` \"\"\"\n",
    "    phase  = np.zeros(window_size)\n",
    "    hanning_window = np.hanning(window_size)\n",
    "    result = np.zeros( int(sound_array.size//f) + window_size)\n",
    "\n",
    "    for i in np.arange(0, sound_array.size-(window_size+h), int(h*f)): # You already know what np.arange does! \n",
    "        \n",
    "        # Create overlapping arrays \n",
    "        a1 = sound_array[i: i + window_size] # First block \n",
    "        a2 = sound_array[i + h: i + window_size + h] # Shifted block \n",
    "        \n",
    "        # Multiply with Hanning window, then fft \n",
    "        s1 =  np.fft.fft(hanning_window * a1) # First block in freq domain\n",
    "        s2 =  np.fft.fft(hanning_window * a2) # Shifted block in freq domain \n",
    "        '''\n",
    "        ----------------------------------------\n",
    "        This part is really MRI sciency!!!!!  \n",
    "        ---------------------------------------\n",
    "        '''\n",
    "        phase = (phase + np.angle(s2/s1)) % 2*np.pi\n",
    "        '''                                            \n",
    "        (2pi X phase_difference) between overlapping blocks\n",
    "        Note that phase differences are tracked across windows (SUM), we are in a for loop. This was \n",
    "        supposed to be done more elegantly. Our pitch corrected signal will sound crispy. \n",
    "        ''' \n",
    "        a2_rephased = abs(np.fft.ifft(np.abs(s2)*np.exp(1j*phase)))\n",
    "        '''\n",
    "        If we multiply the shifted frequency block by e^(2pi * i * phase_offset)...\n",
    "        we correct for the phase difference, sounds familiar? :))  \n",
    "        '''\n",
    "        # /Stretch/Accumulate/arrange outputs\n",
    "        i2 = int(i/f)\n",
    "        result[i2 : i2 + window_size] += hanning_window*a2_rephased \n",
    "        \n",
    "    result = ((2**(16-4)) * result/result.max()) # normalize (16bit)\n",
    "    return result.astype('int16')\n",
    "\n",
    "def pitchshift(snd_array, n, window_size=2**13, h=2**11): \n",
    "    \"\"\" Changes the pitch of a sound by ``n`` semitones. \"\"\"\n",
    "    factor = 2**(1.0 * n / 12.0)\n",
    "    stretched = stretch(snd_array, 1.0/factor, window_size, h)\n",
    "    return fastforward(stretched[window_size:], factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "WARNING \n",
    "This implementation is not robust & really functional at all. It probably \n",
    "won't even work for other audio files. \n",
    "'''\n",
    "pitch_me  = load_audio('WavMRI/EPI.wav') \n",
    "pitched = pitchshift(pitch_me['signal'],3)\n",
    "make_audio(pitched,44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3> A really important lesson about scientific computing in Python </h3></div>\n",
    "\n",
    "Python is a full spectrum programming language. As of today, there are 235,000 packages served by `PyPI` (`pip`) alone! \n",
    "\n",
    "### If you know the right keywords, make an informed Google search. More often than not, you'll find a Python package for what you are looking for.\n",
    "\n",
    "<center><img src=\"http://cecilialiao.com/wp-content/uploads/2015/06/mlwgc.jpg\"></center>\n",
    "\n",
    "#### Given that I was not happy with the `pitchshift` implementation above, I did my Google search and found an amazing package tailored for music DSP: \n",
    "\n",
    "<center><img src=\"https://librosa.org/doc/latest/_static/librosa_logo_text.svg\"></center>\n",
    "\n",
    "<br><center> A chef specialized in the audio cuisine!  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "'''\n",
    "CHANGE THE PITCH WITHOUT AFFECTING THE ORIGINAL DURATION \n",
    "'''\n",
    "\n",
    "vocal  = load_audio('WavVocal/SUNRISE_BENGU.wav')\n",
    "vocal_octave = librosa.effects.pitch_shift(vocal['signal'], vocal['fsamp'], n_steps=-7)\n",
    "make_audio(vocal_octave,vocal['fsamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CHANGE THE DURATION WITHOUT AFFECTING THE ORIGINAL PITCH\n",
    "No chipmunk transformation :) \n",
    "'''\n",
    "\n",
    "vocal_slow = librosa.effects.time_stretch(vocal['signal'], 1.5)\n",
    "make_audio(vocal_slow,vocal['fsamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h3> Singing Sunrise in the perfect fifth: A Python-powered COVID-safe duet </h3></div>\n",
    "\n",
    "![](assets/harmony.png)\n",
    "\n",
    "Do not miss out this [musical explanation of spherical harmonics](https://www.youtube.com/watch?v=EcKgJhFdtEY).\n",
    "\n",
    "![](assets/circle_of_fifths.png)\n",
    "\n",
    "### The term \"singing in the perfect fifth\" refers to two vocalists performing a duet 7 semitones apart. \n",
    "\n",
    "If I were singing along with Bengu, my key would have been low-pitched. Yet, I would not be able to sing from one octave lower (12 semitones), I am not that good of a singer.\n",
    "\n",
    "Although it violates the rules of perfect `resonance` as we know, there's a `second perfect harmonic tone` (any many other harmonics) in music!  \n",
    "\n",
    "When we down-pitch Bengu's voice by 7 semitones, we transpose the key from `A#` to `D#` (`IV` in the figure above). Conversely, when we go 7 halftones up, we transpose the key from `A#` to `E#` (`V` in the figure above). Both `D#` and `E#` are in the `major` scale, hence the circle of fifths! Surprise, surprise, both notes are in the chord progression of the original song as well :) \n",
    "\n",
    "Let's pitch Bengu's voice by 7 halftones using `librosa` and pretend that's me. We already know how to overlay multiple tracks into one to turn it into a duet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vocal_agah = librosa.effects.pitch_shift(vocal['signal'], vocal['fsamp'], n_steps=-7)\n",
    "\n",
    "# Well, we'd need a Lyric Soprano for this\n",
    "# vocal_brightman = librosa.effects.pitch_shift(vocal['signal'], vocal['fsamp'], n_steps=12)\n",
    "\n",
    "make_audio(vocal['signal'] + vocal_agah,vocal['fsamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "And finally, when we add all the layers together...\n",
    "'''\n",
    "\n",
    "duet = load_audio('REMIX/SUNRISE_VERSE1_DUET.wav')\n",
    "duet['play']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <h2> üíÅ‚Äç‚ôÄÔ∏èüéô + üé∏ + üß≤üîä + üíª  </h2><h3>THE Exercise</h3> </div>\n",
    "\n",
    "\n",
    "#### Use functions we wrote in this notebook, `librosa.effects.pitch_shift()`, `librosa.effects.time_stretch()` and your `NumPy` skills to make your own remix of MRI-flavored Sunrise! So many possible combinations: \n",
    "\n",
    "| Folder | Number of files| Description|\n",
    "|:-------|:--------|:-------|\n",
    "|WavIR| 38| Time-domain impulse responses for different reverb effects.|\n",
    "|WavMRI| 25 | Acoustic noise of various pulse sequences and some other sounds from the MRI room. | \n",
    "|WavGuitar| 2 | Guitar layer of Sunrise. (Verse1 and the whole song)|\n",
    "|WavVocal| 2 | Vocals & back-vocals recorded by Bengu Aktas (Verse 1 and the whole song). |\n",
    "\n",
    "### Chord durations for verse 1 \n",
    "![](assets/sunrise_chords.png)\n",
    "\n",
    "Time intervals are given with reference to `WavGuitar/SUNRISE_GUITAR.wav`.\n",
    "\n",
    "<code style=\"background:red;color:white\"> <b>You'll need to use the following information to remix using 3D-SPGR. If you'd like to use other sequences, you'll need to come up with your own pitch correction steps! </b></code>\n",
    "\n",
    "1. Using a single sequence file (e.g., `AFI.wav` for all chords), just find out how many steps you need to pitch up/down with reference to `A#2 (116Hz)` (hint: see `Test your musical ear` exercise above). You can transpose the remainign chords from there as you 'instrument' is tuned :) You can find respective frequencies in the table below.\n",
    "\n",
    "\n",
    "2. If you'd like to use different sequence sounds for different chords (e.g. `SHIM.wav` for `A#2` and `LOC_SSFP.wav` for D#3), then you'll need to repeat (1) for each wav file. \n",
    "\n",
    "\n",
    "### Pitch correction factors for `3D SPGR`\n",
    "\n",
    "<code style=\"background:yellow;color:black\"> <b>The following table and color-annotated lyrics of the first verse contain neccesary information to harmoniously remix Sunrise guitar & vocal tracks with the acoustic noise of NORAH JONES sequence made at each TR:</b></code>\n",
    "\n",
    "\n",
    "|Tonic Note  | Frequency   |Attuned?|Pitch Shift|Color Code|Wav file|\n",
    "|:-----|:-----------|:--------|:----------------|:----------|:--------|\n",
    "|A#2         | 116 Hz      |   üò¢   | ‚¨ÜÔ∏è  3 halftones*           | Red      |`3DSPGR_TR13ms.wav`|\n",
    "|D#3         | 155 Hz      |   üéâ   | -              | Blue     |`3DSPGR_TR12ms.wav`|\n",
    "|C3          | 130 Hz      |   üéâ   | -              |Purple     |`3DSPGR_TR10ms.wav`|\n",
    "|G#2         | 103 Hz      |   üò¢   | ‚¨áÔ∏è  1 halftone  |Green     |`3DSPGR_TR20ms.wav`|\n",
    "\n",
    "### Share your art & the code to re-generate it \n",
    "\n",
    "Once you are happy with your masterpiece, share it with us! \n",
    "\n",
    "1. Clone this repository \n",
    "2. Create a new directory under the `REMIX` folder. For example `REMIX/your_name`.\n",
    "3. Put your `*.wav` output and the notebook that generates it in the folder. \n",
    "4. Commit your changes \n",
    "5. Send a Pull Request to `agahkarakuzu/sunrise` \n",
    "\n",
    "Who knows, maybe we can play them at the Highlights Party next year in London! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"background-color:#1e3059;border-radius:30px 10px\">\n",
    "<h1 style=\"color:white;margin-top:1px;\">  Other fun resources to learn Python basics for scientific computing</h1></div>\n",
    "\n",
    "<center><h3> <a href=\"https://earsketch.gatech.edu/landing/#/learn\">EarSketch</a> is a must see!</h3></center>\n",
    "\n",
    "![](assets/ear_sketch.png)\n",
    "\n",
    "<center><h3 style=\"color:purple\"> Create an account, use any wav file you like from this repository and improve your Python skills as you make music with MRI sounds!</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## [AllenDowney/ThinkDSP](https://github.com/AllenDowney/ThinkDSP) \n",
    "\n",
    "This GitHub repository is full of notebooks dedicated for digital signal processing in Python. Allen Downey's [SciPy 2015 talk](https://www.youtube.com/watch?v=0ALKGR0I5MA) was a great source of inspiration for me to come up with this course. Thank you Allen! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Time to say goodbye to 1D with an impeccable harmony \n",
    "\n",
    "We dealth with `acoustic dissonance` quite a bit. Let's move on with `magnetic resonance`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/g3ENX3aHlqU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb35e2096d8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/g3ENX3aHlqU',560,315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
